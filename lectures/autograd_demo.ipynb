{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2c0983f",
   "metadata": {},
   "source": [
    "# Simple example\n",
    "https://pytorch.org/tutorials/beginner/blitz/autograd_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "44f2be87",
   "metadata": {},
   "outputs": [],
   "source": [
    "### \n",
    "import torch\n",
    "\n",
    "a = torch.tensor([2.,3.],requires_grad = True)\n",
    "b = torch.tensor([6.,4.],requires_grad = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4654ed6",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "Q = 3a^3 - b^2\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "896d535c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 3*a**3 - b**2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05b933d8",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\frac{\\partial Q}{\\partial a} &= 9a^2\\\\\n",
    "\\frac{\\partial Q}{\\partial b} &= -2b\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abc2dcad",
   "metadata": {},
   "source": [
    "When we call `.backward()` on `Q`, autograd calculates these gradients and stores them in the respective tensorsâ€™ `.grad` attribute. However, the `.backward()` method expects the object to be a scalar by default, so we first aggregate `Q` and then call backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "71251817",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q.sum().backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5e5ad16b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of Q wrt a: \n",
      "analytical:  tensor([36., 81.], grad_fn=<MulBackward0>)\n",
      "autograd:  tensor([36., 81.])\n",
      "gradient of Q wrt b: \n",
      "analytical:  tensor([-12.,  -8.], grad_fn=<MulBackward0>)\n",
      "autograd:  tensor([-12.,  -8.])\n"
     ]
    }
   ],
   "source": [
    "print('gradient of Q wrt a: ')\n",
    "print('analytical: ', 9*a**2)\n",
    "print('autograd: ', a.grad)\n",
    "\n",
    "print('gradient of Q wrt b: ')\n",
    "print('analytical: ', -2*b)\n",
    "print('autograd: ', b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1430ebe8",
   "metadata": {},
   "source": [
    "# Slightly more complicated example\n",
    "\n",
    "https://pytorch.org/tutorials/beginner/basics/autogradqs_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1e8543",
   "metadata": {},
   "source": [
    "![](https://pytorch.org/tutorials/_images/comp-graph.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "652653d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1.])\n",
      "tensor([0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(5)  # input tensor\n",
    "y = torch.zeros(3)  # expected output - 3 dimensional\n",
    "\n",
    "print(x)\n",
    "print(y)\n",
    "\n",
    "# parameters\n",
    "w = torch.randn(5, 3, requires_grad=True)\n",
    "b = torch.randn(3, requires_grad=True)\n",
    "\n",
    "# linear predictors\n",
    "z = torch.matmul(x, w)+b\n",
    "\n",
    "# cross-entropy loss for unnomalized pre-activations\n",
    "loss = torch.nn.functional.binary_cross_entropy_with_logits(z, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438e495f",
   "metadata": {},
   "source": [
    "To optimize the parameters in the network, we compute the derivatives of our loss function with respect to parameters. Specifically, we are concerned with:\n",
    "\\begin{align*}\n",
    "&\\frac{\\partial L}{\\partial w} \\textrm{ and } \\frac{\\partial L}{\\partial b}\n",
    "\\end{align*}\n",
    "\n",
    "at some `x` and `y`. To compute those derivatives, we again call `loss.backward()`, which can be retrieved respectively via `w.grad` and `b.grad`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2ccfa78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0505, 0.1686, 0.3321],\n",
      "        [0.0505, 0.1686, 0.3321],\n",
      "        [0.0505, 0.1686, 0.3321],\n",
      "        [0.0505, 0.1686, 0.3321],\n",
      "        [0.0505, 0.1686, 0.3321]])\n",
      "tensor([0.0505, 0.1686, 0.3321])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(w.grad)\n",
    "print(b.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f542146",
   "metadata": {},
   "source": [
    "# Autograd in training\n",
    "https://pytorch.org/tutorials/beginner/introyt/autogradyt_tutorial.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "da7426df",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "DIM_IN = 784 \n",
    "HIDDEN_SIZE = 1000\n",
    "DIM_OUT = 10\n",
    "\n",
    "class Model(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.layer1 = torch.nn.Linear(DIM_IN, HIDDEN_SIZE)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.layer2 = torch.nn.Linear(HIDDEN_SIZE, DIM_OUT)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.layer1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "\n",
    "# instanstiate the model\n",
    "model = Model()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8191001b",
   "metadata": {},
   "source": [
    "Within a subclass of `torch.nn.Module`, gradients are tracked by default, so we need not specify `requires_grad = True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a44929ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(model.layer2.weight.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b0c29d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate some data\n",
    "some_input = torch.randn(BATCH_SIZE, DIM_IN, requires_grad=False)\n",
    "ideal_output = torch.randn(BATCH_SIZE, DIM_OUT, requires_grad=False)\n",
    "\n",
    "# generate a prediction\n",
    "prediction = model(some_input)\n",
    "\n",
    "# associate the model's parameters with an optimizer, in this case SGD\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "\n",
    "# define a loss function\n",
    "loss = (ideal_output - prediction).pow(2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "50d4fd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one training step\n",
    "loss.backward()\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed903f6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import MSELoss\n",
    "\n",
    "# reinitialize the model\n",
    "model = Model()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.001)\n",
    "loss_func = MSELoss()\n",
    "\n",
    "# several passes through the dataset\n",
    "for i in range(0, 100):\n",
    "    prediction = model(some_input)\n",
    "    \n",
    "    loss = loss_func(prediction,ideal_output)\n",
    "    # print(loss.item())\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # the optimizer accumulates gradients by default; reset to zero or else it will blow up\n",
    "    optimizer.zero_grad(set_to_none=False)\n",
    "    # print(model.layer2.weight.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "69f05d9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.0155)\n"
     ]
    }
   ],
   "source": [
    "# To evaluate the trained model, we turn off autograd because we don't need it\n",
    "some_input_test = torch.randn(BATCH_SIZE, DIM_IN, requires_grad=False)\n",
    "with torch.no_grad():\n",
    "    prediction = model(some_input_test)\n",
    "    val_loss = loss_func(prediction,ideal_output)\n",
    "    print(val_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
