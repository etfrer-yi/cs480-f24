{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "decdeeaa-532e-45e4-981d-95ede850a330",
   "metadata": {},
   "source": [
    "# Instructions\n",
    "\n",
    "- This assignment was posted on 23 October, 2024 and is due on 8 November 2024, at 11:59 pm\n",
    "- Answer all questions in this Jupyter notebook skeleton within the provided cells. Questions will indicate whether the answer should take the form of a coded or written response. Use the dropdown menu within the Jupyter interface to toggle between 'Markdown' or 'Code' for the cells. Do NOT delete or rearrange any of the question blocks within this skeleton.\n",
    "- The following two files should be submitted to LEARN:\n",
    "    - This IPYNB file containing the questions and your answers in either code or markdown.\n",
    "    - A PDF printout of this IPYNB file. To generate this, first run and save the output of all cells. Then expand all cells and print as PDF. Be sure that all your code and answers are visible in the PDF document you submit. \n",
    "- The total number of marks for this assignment is 100."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daca74f9",
   "metadata": {},
   "source": [
    "## Exercise 1. Decision Trees\n",
    "\n",
    "### EXERCISE1-TASK1: [10 marks]\n",
    "Implement a decision tree algorithm from scratch using a binary splitting structure, taking the Gini Index as the loss.\n",
    "\n",
    "To receive full marks, your code should meet the following requirements:\n",
    "- Allow for classification in the multiclass setting. Note that the Gini Index loss covered in class \n",
    "\\begin{align}\n",
    "l = \\sum_{k=1}^c \\hat p_{mk}(1 - \\hat p_{mk})\n",
    "\\end{align}\n",
    "where $\\hat p_{mk}$ is the empirical fraction of samples in some region $m$ of the space belonging to class $k$ out of $c$ total class labels, is general to the number of classes c.\n",
    "- The maximum depth of the decision tree can be controlled through a user-selected parameter\n",
    "- The implementation is positioned for application as part of the Random Forest algorithm, and implements two feature subsampling methods, to randomly select from $m < p$ input features for determining the best split, for each split in the tree-building process. In other words, each split evaluate only the selected subset of features. Specifically, allow for the user to select from $m=p$ (no subsampling), $m = p/2$, and $m=\\sqrt{p}$ features.\n",
    "- A stopping criterion has been selected from those covered in class, such as reaching the maximum depth, pure leaf nodes, or a minimal number of samples. You may also apply mutiple stopping criteria together, as is often done in practice. Add a comment in your code to indicate which criteria you have chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "143cf5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOLUTION BLOCK ###\n",
    "### YOUR CODE GOES HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f94a4c2",
   "metadata": {},
   "source": [
    "### EXERCISE1-TASK2 [10 marks]:\n",
    "Fit a decision tree to the Iris dataset, using a 67%/33% train/test split. Visualize the performance of your tree on the test dataset using a multiclass confusion matrix (you may use methods from `sklearn.metrics` for this part. Compare your performance to the implementation from `sklearn.tree.DecisionTreeClassifier`, and comment on any discrepancies in behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba7d1493",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOLUTION BLOCK ###\n",
    "### YOUR CODE GOES HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde51023",
   "metadata": {},
   "source": [
    "### EXERCISE1-TASK3 [10 marks]:\n",
    "\n",
    "Explore the performance of your Random Forest algorithm implementation on the `heart.csv` dataset using your implementation from **EXERCISE1-TASK1**, over a range of number of trees and with different methods of feature subsampling.\n",
    "\n",
    "Detailed instructions: First, begin by loading the heart disease dataset stored in `heart.csv`. Note that this dataset has categorial columns (\"cp\" and \"restecg\"), that must be converted to one-hot encoded columns. The labels, that indicate the presence or absence of heart disease, are stored in the \"target\" column that you will aim to predict. Split the dataset into disjoint training set (66%) and a testing set (33%).\n",
    "\n",
    "Next, explore a Random Forest with each of `[1,2,5,10,20,50,100,200,500,1000]` number of trees, and for each of the cases of $m = p$, $m = p/2$, and $m = \\sqrt(p)$. Set the maximum depth for an individual tree to 5. For each combination of hyperparameter settings, build your decision tree using the training set and evaluate its performance on the testing dataset, and store the test classification error for later plotting.\n",
    "\n",
    "Generate a plot of `Test Classification Error` as a function of `Number of Trees`, with separate line plots for each of the three feature subsampling methods. (e.g. the setup of your figure should correspond to that of Figure 8.10 in *Introduction to Statistical Learning*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb2f7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOLUTION BLOCK ###\n",
    "### YOUR CODE GOES HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1407abe",
   "metadata": {},
   "source": [
    "## Exercise 2. Bias-Variance Decomposition\n",
    "In class, we analyzed a regression problem of predicting a continuous random variable $Y \\in \\mathbb{R}$ which is an unknown function $f: \\;\\; \\mathbb{R}^d \\to \\mathbb{R}$ of $X \\in \\mathbb{R}^d$ corrupted by Gaussian noise $\\epsilon \\sim \\mathcal{N}(\\mu = 0, \\sigma_{\\epsilon}^2)$\n",
    "\\begin{align*}\n",
    "Y &= f(X) + \\epsilon\\\\\n",
    "\\end{align*}\n",
    "and saw how the implications of decomposing expected error on the test dataset into the three components that we labelled irreducible error, squared bias, and variance.\n",
    "\n",
    "### **EXERCISE2-TASK1: [10 marks]**\n",
    "For this regression setup, show that the expectation of the squared error for some new test input $x_t$:\n",
    "\\begin{align*}\n",
    "\\textrm{Err}(x_t) &= \\mathbb{E}\\left[ (Y - \\hat{f}(x_t))^2 \\mid X = x_t \\right] \\\\\n",
    "\\end{align*}\n",
    "May be expressed as\n",
    "\\begin{align*}\n",
    "\\textrm{Err}(x_t) &= \\sigma_{\\epsilon}^2 + \\left[ \\mathbb{E}[\\hat{f}(x_t)] - f(x_t) \\right]^2 + \\mathbb{E}\\left[ \\left( \\hat{f}(x_t) - \\mathbb{E}[\\hat{f}(x_t)] \\right)^2 \\right].\n",
    "\\end{align*}\n",
    "\n",
    "Where $\\hat f(x)$ denotes our approximated function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b39fed3",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------- Solution Block ----------------------------------------------------------------------------------------------\n",
    "\n",
    "Write your solution here using the built-in support for MathJax and Markdown.\n",
    "\n",
    "----------------------------------------------------------------------------------------------- Solution Block ----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6efdc6",
   "metadata": {},
   "source": [
    "## Exercise 3. Bagging with Correlated Samples\n",
    "(Adapted from Exercise 15.1 in *Elements of Statistical Learning*) A critical assumption underpinning bootstrap aggregation is that pairwise correlations between model errors is low. However, independent bootstrap realizations will not be perfectly decorrelated, and so the actual variance reduction achieved in practice will be much less. In this task you will explore the variance reduction when samples are not independent, that is, they are correlated. \n",
    "\n",
    "### **EXERCISE3-TASK1: [10 marks]**\n",
    "In class, we found that given $Bn$ independent draws from a normal distribution (i.e. $x_i \\sim_{i.i.d} \\mathcal{N}(\\mu,\\sigma^2)$), the variance of the estimate of the mean was $\\frac{1}{Bn}\\sigma^2$.\n",
    "Suppose you have a set of $B$ random variables that are identically distributed, but not independent; that is, they have some positive pairwise correlation coefficient $\\rho > 0$. Show that the variance of the estimate of the mean of $B$ random variables is given by:\n",
    "\\begin{align*}\n",
    "\\rho \\sigma^2 + \\frac{1-p}{B}\\sigma^2\n",
    "\\end{align*}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "373ad32b",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------- Solution Block ----------------------------------------------------------------------------------------------\n",
    "\n",
    "Write your solution here using the built-in support for MathJax and Markdown.\n",
    "\n",
    "----------------------------------------------------------------------------------------------- Solution Block ----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871cb4e6",
   "metadata": {},
   "source": [
    "## Exercise 4. Gaussian Mixture Models\n",
    "\n",
    "### EXERCISE4-TASK 1 [10 marks]:\n",
    "Qualitatively reproduce Figure 9.5 in *Pattern Recognition and Machine Learning*. That is, generate a 3-panel plot to visualize samples from the a) joint distribution and b) marginal distributions of a 3-component, 2D gaussian mixture model, as well as c) the \"responsibility\" of each component $k$ in explaining each sample drawn.\n",
    "\n",
    "You may use `scipy.stats.multivariate_normal` to define the PDF and draw samples from individual components. Assume the following parameters for the GMM:\n",
    "\\begin{align}\n",
    "\\mu_1 &= (0.5,0.5), \\Sigma_1 = \\begin{bmatrix} 0.02 & -0.01 \\\\ -0.01 & 0.02 \\end{bmatrix}\\\\\n",
    "\\mu_2 &= (0.8,0.6), \\Sigma_1 = \\begin{bmatrix} 0.02 & 0.01 \\\\ 0.01 & 0.02 \\end{bmatrix}\\\\\n",
    "\\mu_3 &= (0.1,0.4), \\Sigma_1 = \\begin{bmatrix} 0.02 & 0.01 \\\\ 0.01 & 0.02 \\end{bmatrix}\\\\\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d8a6e228",
   "metadata": {},
   "outputs": [],
   "source": [
    "### SOLUTION BLOCK ###\n",
    "### YOUR CODE GOES HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12a26005",
   "metadata": {},
   "source": [
    "## Exercise 5. Multilayer Perceptrons\n",
    "\n",
    "\n",
    "### EXERCISE5-TASK1 [10 marks]:\n",
    "Compute the derivative of the Mish activation function with respect to $x$\n",
    "\\begin{align}\n",
    "\\textrm{mish}(x) = x \\cdot \\textrm{tanh}( \\textrm{log}(1+e^x) )\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b3e53",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------- Solution Block ----------------------------------------------------------------------------------------------\n",
    "\n",
    "Write your solution here using the built-in support for MathJax and Markdown.\n",
    "\n",
    "----------------------------------------------------------------------------------------------- Solution Block ----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59389d7e",
   "metadata": {},
   "source": [
    "### EXERCISE5-TASK2 [10 marks]:\n",
    "(Adapted from Exercise 5.1, *Dive into Deep Learning*) In class, we saw how adding layers to a linear deep network (a network without a nonlinearity) can never increase the expressive power of a network. Give an example where adding layers to a linear deep network reduces the expressive power of a network. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e109efbc",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------- Solution Block ----------------------------------------------------------------------------------------------\n",
    "\n",
    "Write your solution here using the built-in support for MathJax and Markdown.\n",
    "\n",
    "----------------------------------------------------------------------------------------------- Solution Block ----------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "620f2585",
   "metadata": {},
   "source": [
    "## Exercise 6. AdaBoost as Fitting an Additive Model\n",
    "\n",
    "Our in-class discussion of AdaBoost did not provide a theoretical rationale for why it works.\n",
    "The AdaBoost algorithm is in fact a specific case of *forward stagewise additive modeling*, which in turn is a way of fitting an *expansion set* of elementary *basis functions*. \n",
    "\n",
    "More generally, expansion set models are central to many advanced learning techniques and take the form\n",
    "\\begin{align}\n",
    "f(x) = \\sum_{m-1}^M \\alpha_m g_m(x, \\theta_m),\n",
    "\\end{align}\n",
    "where $M$ is the number of basis functions, $\\alpha_m$ is the weight of the $m^{th}$ classifier, and $\\theta$ are its parameters. \n",
    "\n",
    "Such models are usually fit by minimizing some loss function $l$ over the training data to find estimates of the set of importance coefficients $\\{ \\hat \\alpha_m\\}$ and basis function parameters $\\{ \\hat \\theta_m\\}$. This is to say that\n",
    "\\begin{align}\n",
    "\\{ \\hat \\alpha_m, \\hat \\theta_m\\}_1^M = \\textrm{argmin}_{\\{ \\alpha_m, \\theta_m\\}_1^M}\\sum_{i=1}^N l\\Bigg(y_i,\\sum_{m-1}^M \\alpha_m g_m(x, \\theta_m)\\Bigg)\n",
    "\\end{align}\n",
    "\n",
    "Forward stagewise additive modelling aproaches, like AdaBoost, instead find an approximate solution to this objective by adding basis functions in sequence. That is, at each iteration $m$, one estimates\n",
    "\\begin{align}\n",
    "\\hat \\alpha_m, \\hat \\theta_m\\ = \n",
    "\\textrm{argmin}_{ \\alpha_m, \\theta_m\\ }\\sum_{i=1}^N l\\Bigg(y_i, f_{m-1} + \\alpha_m g_m(x, \\theta_m)\\Bigg)\n",
    "\\end{align}\n",
    "which positions one to make predictions for $x$\n",
    "\\begin{align}\n",
    "f_m(x) = f_{m-1}(x) + \\alpha_m g_m(x, \\theta_m)\n",
    "\\end{align}\n",
    "In the case of applying AdaBoost to learn a binary classifier, these basis functions would be the base classifiers or members of the ensemble.\n",
    "\n",
    "### EXERCISE6-TASK1 [20 marks]:\n",
    "Show how the multiplicative weight update of AdaBoost\n",
    "\\begin{align}\n",
    "w_i^{(m+1)} = w_i^{(m)}e^{\\alpha_m \\mathbb{1}(y_i \\neq g_m(x_i))}\n",
    "\\end{align}\n",
    "can be derived from the forward stagewise additive modelling technique for the special case of minimizing an exponential loss function\n",
    "\\begin{align}\n",
    "l(y,f(x)) = \\sum_{i=1}^N e^{-y_if(x_i)}\n",
    "\\end{align}\n",
    "where $y \\in \\{ -1, +1\\}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0a2d46d",
   "metadata": {},
   "source": [
    "----------------------------------------------------------------------------------------------- Solution Block ----------------------------------------------------------------------------------------------\n",
    "\n",
    "Write your solution here using the built-in support for MathJax and Markdown.\n",
    "\n",
    "----------------------------------------------------------------------------------------------- Solution Block ----------------------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
